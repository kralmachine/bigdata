{"paragraphs":[{"title":"Giriş","text":"%md\n\nİngiliz bilimci Sir Francis Galton yaptığı ölçümlerde ortalamanın çok üzerinde iri, uzun veya kısa, küçük ailelerin çocuklarının da uzun veya kısa olduğunu ancak genel ortalama ile aile arasında kaldığını ölçtü. Yani upuzun bir basketbolcunun oğlu da ortalamadan daha uzun oluyordu ancak kendisindan daha uzun değil. Böylelikle insanların fiziksel özellikleri insanlık ortalamasına doğru itiliyordu. Bunu grafiğe döktüğünde ortaya çıkan çizginin doğrusal olduğunu farketti. Eğim pozitif ve birden azıcık küçüktü. Galton bu olayı ortalamaya doğru regresyon diye ifade etti.","user":"admin","dateUpdated":"2017-12-25T00:02:05+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":true,"tableHide":false,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>İngiliz bilimci Sir Francis Galton yaptığı ölçümlerde ortalamanın çok üzerinde iri, uzun veya kısa, küçük ailelerin çocuklarının da uzun veya kısa olduğunu ancak genel ortalama ile aile arasında kaldığını ölçtü. Yani upuzun bir basketbolcunun oğlu da ortalamadan daha uzun oluyordu ancak kendisindan daha uzun değil. Böylelikle insanların fiziksel özellikleri insanlık ortalamasına doğru itiliyordu. Bunu grafiğe döktüğünde ortaya çıkan çizginin doğrusal olduğunu farketti. Eğim pozitif ve birden azıcık küçüktü. Galton bu olayı ortalamaya doğru regresyon diye ifade etti.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514139994002_851669135","id":"20171224-202634_472315100","dateCreated":"2017-12-24T20:26:34+0200","dateStarted":"2017-12-25T00:02:05+0200","dateFinished":"2017-12-25T00:02:07+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:1293"},{"title":"Fast Forward to Regression","text":"%md\nRegression genel olarak büyüklük, gelir veya sıcaklık gibi nümerik değeri tahmin etmektle ilgilidir. Kategorik bir değeri tahmin etmekle ilgilidir, Resimdeki kedidir veya değildir. Hem regresyon hem de sınıflandırma güdümlü (supervised) yöntemler altındadır.","user":"admin","dateUpdated":"2017-12-25T00:02:09+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Regression genel olarak büyüklük, gelir veya sıcaklık gibi nümerik değeri tahmin etmektle ilgilidir. Kategorik bir değeri tahmin etmekle ilgilidir, Resimdeki kedidir veya değildir. Hem regresyon hem de sınıflandırma güdümlü (supervised) yöntemler altındadır.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514140403631_-1036327655","id":"20171224-203323_1771828614","dateCreated":"2017-12-24T20:33:23+0200","dateStarted":"2017-12-25T00:02:09+0200","dateFinished":"2017-12-25T00:02:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1294"},{"title":"Vectors and Features","text":"%md\nBugünün havasıyla yarınınkini tahmin etmek. Bunun için nitelikler lazım. Özellik, nitelik, boyut, prediktör, ya da değişken. Bugünle ilgili elimizde şu bilgiler olsun: high temperature 13.1, low temperature 19.0, average humidity 0.73, have genel durumu bulutlu, prediktör 1. Bu beş niteliği yanyana yazdığımızda 13.1, 19.0, 0.73, bulutlu, 1 feature vector olur ve bu bilgi ile herhangi bir güne ait havayı tanımlayabiliriz. Buradaki özelliklerin hepsi aynı türden değil. İlk ikisi derece, üçüncüsünde birim yok, dördüncü rakam bile değil, beşinci ise her zaman pozitif tam sayı.\n\nNitelikler kabaca ikiye ayrılır kategorik ve nümerik. ","user":"admin","dateUpdated":"2017-12-25T00:02:10+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Bugünün havasıyla yarınınkini tahmin etmek. Bunun için nitelikler lazım. Özellik, nitelik, boyut, prediktör, ya da değişken. Bugünle ilgili elimizde şu bilgiler olsun: high temperature 13.1, low temperature 19.0, average humidity 0.73, have genel durumu bulutlu, prediktör 1. Bu beş niteliği yanyana yazdığımızda 13.1, 19.0, 0.73, bulutlu, 1 feature vector olur ve bu bilgi ile herhangi bir güne ait havayı tanımlayabiliriz. Buradaki özelliklerin hepsi aynı türden değil. İlk ikisi derece, üçüncüsünde birim yok, dördüncü rakam bile değil, beşinci ise her zaman pozitif tam sayı.</p>\n<p>Nitelikler kabaca ikiye ayrılır kategorik ve nümerik.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514141275570_-31694082","id":"20171224-204755_1070736044","dateCreated":"2017-12-24T20:47:55+0200","dateStarted":"2017-12-25T00:02:10+0200","dateFinished":"2017-12-25T00:02:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1295"},{"title":"Training Examples","text":"%md\nHava durumuna ait nitelikler ile bir sonraki günün hava sıcaklığı tahmin edilecekse regresyon, hava durumu tahmin edilecekse sınıflandırma kullanılır. ","user":"admin","dateUpdated":"2017-12-25T00:02:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true,"editorHide":true,"tableHide":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514142447405_-1529680846","id":"20171224-210727_1293961026","dateCreated":"2017-12-24T21:07:27+0200","dateStarted":"2017-12-25T00:02:11+0200","dateFinished":"2017-12-25T00:02:11+0200","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:1296"},{"title":"Decision Trees and Forests","text":"%md\nKarar ağaçları hem kategorik hem nümerik özelliklerle çalışabilir. Sıra dışı değerlerden çok fazla etkilenmezler. Değişik tür ve ölçekte niteliklerle çalışırlar, normalizasyon gerektirmezler. Bu ünitede veri setine DecisionTree ve RandomForest modelleri uygulanacaktır. Aslında insanlar olarak bizler de karar ağaçlarında izlenen mantığı günlük hayatımızda farkında olmadan kullanırız. Sütün  bozulup bozulmadığının kontrolü mesela; önce tarihe bak, tarih yeni geçmişse kokla, bişey anlamadıysan birazcık tat. Her bir aşama da cevabın evet veya hayır olacak ve ona göre farklı davranacaksın.  \n\nEvcil hayvan dükkanına alınan robot. Robot, dükkan açılmadan hangi hayvanın çocuk için iyi bir hayvan olacağını tahmin etmeye çalışıyor. Dükkan sahibi hayvanon adı, ağırlığı, kaç ayaklı olduğu, rengi ve iyi bir hayvan oluğ olmadığına dair bir liste hazırlıyor. Bu özelliklerden ismin sonucu tahmin etmede hiçbir etkisi olmadığı açık. Çünkü isim herhangi bir hayvana ait olabilir. Öyleyse geriye üç özellik kalıyor (ağırlık ve ayak sayısı, nümerik) renk ise kategorik. ","user":"admin","dateUpdated":"2017-12-25T00:02:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Karar ağaçları hem kategorik hem nümerik özelliklerle çalışabilir. Sıra dışı değerlerden çok fazla etkilenmezler. Değişik tür ve ölçekte niteliklerle çalışırlar, normalizasyon gerektirmezler. Bu ünitede veri setine DecisionTree ve RandomForest modelleri uygulanacaktır. Aslında insanlar olarak bizler de karar ağaçlarında izlenen mantığı günlük hayatımızda farkında olmadan kullanırız. Sütün bozulup bozulmadığının kontrolü mesela; önce tarihe bak, tarih yeni geçmişse kokla, bişey anlamadıysan birazcık tat. Her bir aşama da cevabın evet veya hayır olacak ve ona göre farklı davranacaksın. </p>\n<p>Evcil hayvan dükkanına alınan robot. Robot, dükkan açılmadan hangi hayvanın çocuk için iyi bir hayvan olacağını tahmin etmeye çalışıyor. Dükkan sahibi hayvanon adı, ağırlığı, kaç ayaklı olduğu, rengi ve iyi bir hayvan oluğ olmadığına dair bir liste hazırlıyor. Bu özelliklerden ismin sonucu tahmin etmede hiçbir etkisi olmadığı açık. Çünkü isim herhangi bir hayvana ait olabilir. Öyleyse geriye üç özellik kalıyor (ağırlık ve ayak sayısı, nümerik) renk ise kategorik.</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514142992273_-37430854","id":"20171224-211632_1229798476","dateCreated":"2017-12-24T21:16:32+0200","dateStarted":"2017-12-25T00:02:11+0200","dateFinished":"2017-12-25T00:02:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1297"},{"title":"Sütun İsimleri Olmaksızın veriyi yükle (Covtype DataSet) df: dataWithoutHeader","text":"// Veri ABD Colorado Eyaleti arsa/parsel bilgilerini içeriyor\n// Her bir parsel için şu özellikler mevcut: rakım(metre), eğim(derece), gölge durumu, toprak türü (aynı zamanda bilinen orman örtüsünden hangisi ile kaplı olduğu) Bu veri setinde parselin özelliklerinden hangi tür ormanla kaplı olduğu tahmin edilecek. Bunun için 54 özellik var. Wilderness_Area'da dört farlı, Soil_Type'da 40 farklı değer var bunlar 1-of-n veya one-hot encodinlik değerler. \n// 581.012 örnek var. Gerçek değerler. Büyük veri sayılmasa da büyük veri ve ölçeklenebilirliğe örnek için yeterli.  Veri basit csv formatında. Veride header yok. Bu sebeple  option(\"header\",\"true\") kullanmıyoruz. Nitelikler, _c0, _c1, _c2.... şeklinde isimlendirilecek.\n\n\nval dataWithoutHeader = spark.read.option(\"inferSchema\",\"true\").option(\"header\",\"false\").csv(\"/user/erkan/veri_setlerim/covtype.data\")","user":"admin","dateUpdated":"2018-01-07T21:26:54+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndataWithoutHeader: org.apache.spark.sql.DataFrame = [_c0: int, _c1: int ... 53 more fields]\n"}]},"apps":[],"jobName":"paragraph_1513885868014_-541744606","id":"20171221-215108_2061658470","dateCreated":"2017-12-21T21:51:08+0200","dateStarted":"2018-01-07T21:27:09+0200","dateFinished":"2018-01-07T21:31:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1298"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1515351915448_-1505207587","id":"20180107-210515_738259273","dateCreated":"2018-01-07T21:05:15+0200","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:3586","text":"dataWithoutHeader.count()","dateUpdated":"2018-01-07T21:05:39+0200","dateFinished":"2018-01-07T21:06:02+0200","dateStarted":"2018-01-07T21:05:39+0200","results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres5: Long = 581012\n"}]}},{"title":"Sütun isimleri","text":"// ++ collections birleştirir. \n\nval colNames = Seq(\n\"Elevation\",\"Aspect\",\"Slope\",\n\"Horizontal_Distance_To_Hyrology\",\"Vertical_Distance_To_Hyrology\",\n\"Horizontal_Distance_To_Roadways\",\n\"Hillshade_9am\",\"Hillshade_Noon\",\"Hillshade_3pm\",\n\"Horizontal_Distance_To_Fire_Points\"\n)++(\n(0 until 4).map(i => s\"Wilderness_Area_$i\")\n)++(\n(0 until 40).map(i => s\"Soil_Type_$i\")\n)++Seq(\"Cover_Type\")","user":"admin","dateUpdated":"2017-12-25T00:02:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"title":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"colNames: Seq[String] = List(Elevation, Aspect, Slope, Horizontal_Distance_To_Hyrology, Vertical_Distance_To_Hyrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soil_Ty..."}]},"apps":[],"jobName":"paragraph_1513885976482_-867642248","id":"20171221-215256_2119860689","dateCreated":"2017-12-21T21:52:56+0200","dateStarted":"2017-12-25T00:02:12+0200","dateFinished":"2017-12-25T00:03:08+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1299"},{"title":"dataWithoutHeader'e Sütun isimleriyle veriyi birleştirelim df: data","text":"val data = dataWithoutHeader.toDF(colNames:_*).withColumn(\"Cover_Type\", $\"Cover_Type\".cast(\"double\"))","user":"admin","dateUpdated":"2017-12-25T00:02:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\ndata: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 53 more fields]\n"}]},"apps":[],"jobName":"paragraph_1513913620870_-24067685","id":"20171222-053340_585720002","dateCreated":"2017-12-22T05:33:40+0200","dateStarted":"2017-12-25T00:02:50+0200","dateFinished":"2017-12-25T00:03:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1300"},{"text":"data.head","user":"admin","dateUpdated":"2017-12-25T00:02:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres108: org.apache.spark.sql.Row = [2596,51,3,258,0,510,221,232,148,6279,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,5.0]\n"}]},"apps":[],"jobName":"paragraph_1513913975936_-2047324233","id":"20171222-053935_41983674","dateCreated":"2017-12-22T05:39:35+0200","dateStarted":"2017-12-25T00:03:09+0200","dateFinished":"2017-12-25T00:03:24+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1301"},{"title":"Veriyi %90 Eğitim, %10 test olacak şekilde bölelim df: trainData ve testData","text":"val Array(trainData, testData) = data.randomSplit(Array(0.9, 0.1))","user":"admin","dateUpdated":"2017-12-25T00:39:27+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\n\ntrainData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\ntestData: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Elevation: int, Aspect: int ... 53 more fields]\n"}]},"apps":[],"jobName":"paragraph_1513914087921_1643541317","id":"20171222-054127_1714020632","dateCreated":"2017-12-22T05:41:27+0200","dateStarted":"2017-12-25T00:03:19+0200","dateFinished":"2017-12-25T00:03:27+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1302"},{"title":"Girdilerin hepsini bir sütunda vector olarak toplama","text":" // Spark MLlib tüm girdileri bir sütun altında türü vektör olarak toplamak istiyor. vector sadece rakamları(double) kabul eder. Bunun için kütüphanede VectorAssembler sınıfı var.\nimport org.apache.spark.ml.feature.VectorAssembler\n\n// Hedef nitelik hariç diğer nitelikleri seç ve inputCols'a ata\nval inputCols = trainData.columns.filter(_!=\"Cover_Type\")\n\n// Yeni bir VectorAssembler nesnesi yarat girdi ve çıktı isimleri ver. Bu şu anlama geliyor. Girdi için bu sütunları kullan. Çıktı ürettiğinde de ismine bunu ver.\n// VectorAssembler MLlib Pipelines API'sinde bir Transformer. \nval assembler = new VectorAssembler().setInputCols(inputCols).setOutputCol(\"featureVector\")\n\n// Az önce oluşturduğumuz assembler nesnesinin (Pipeline literatüründe aynı zamanda o bir Transformator) transform metoduyla eğitim dataframe'i olan trainData'yı dönüştür. \n// Burada yapılan iş input nitelikleri alarak vector türünde yeni bir sütuna aktarmaktır.\n// Bu yeni nitelikle birlikte elbette dataframe şeması değişeceğinden bu yeni dataframe'i yeni bir isimle tutmak lazım ona da assembledTrainData.\nval assembledTrainData = assembler.transform(trainData)\n\n// yeni dataframemimiz olan assembledTrainData'dan sadece yeni niteliğimizi featureVector seçiyoruz. show ile gösteriyoruz. sütun kesmesi olmasın hepsini göstersin diye truncate=false diyoruz.\nassembledTrainData.select(\"featureVector\").show(truncate = false)","user":"admin","dateUpdated":"2018-01-07T20:23:39+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.feature.VectorAssembler\ninputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hyrology, Vertical_Distance_To_Hyrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soi...\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_aee6cd353b4b\n\nassembledTrainData: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 54 more fields]\n+----------------------------------------------------------------------------------------------------+\n|featureVector                                                                                       |\n+----------------------------------------------------------------------------------------------------+\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1859.0,18.0,12.0,67.0,11.0,90.0,211.0,215.0,139.0,792.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1860.0,18.0,13.0,95.0,15.0,90.0,210.0,213.0,138.0,780.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1863.0,37.0,17.0,120.0,18.0,90.0,217.0,202.0,115.0,769.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1866.0,23.0,14.0,85.0,16.0,108.0,212.0,210.0,133.0,819.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1867.0,20.0,15.0,108.0,19.0,120.0,208.0,206.0,132.0,808.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1868.0,27.0,16.0,67.0,17.0,95.0,212.0,204.0,125.0,859.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1871.0,22.0,22.0,60.0,12.0,85.0,200.0,187.0,115.0,792.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,36.0,19.0,134.0,26.0,120.0,215.0,194.0,107.0,797.0,1.0,1.0])|\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1871.0,37.0,19.0,120.0,29.0,90.0,216.0,195.0,107.0,759.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1872.0,12.0,27.0,85.0,25.0,60.0,182.0,174.0,118.0,577.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,16.0,95.0,22.0,124.0,212.0,205.0,126.0,847.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1872.0,27.0,21.0,108.0,30.0,67.0,206.0,190.0,112.0,713.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,14],[1872.0,35.0,21.0,120.0,18.0,85.0,213.0,189.0,104.0,797.0,1.0,1.0]) |\n|(54,[0,1,2,5,6,7,8,9,13,18],[1874.0,18.0,14.0,90.0,208.0,209.0,135.0,793.0,1.0,1.0])                |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1876.0,29.0,19.0,124.0,34.0,90.0,210.0,195.0,115.0,750.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,19.0,18.0,85.0,25.0,108.0,204.0,199.0,127.0,886.0,1.0,1.0]) |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,18],[1877.0,27.0,24.0,90.0,18.0,95.0,201.0,179.0,104.0,780.0,1.0,1.0])  |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1877.0,28.0,22.0,127.0,35.0,85.0,205.0,185.0,107.0,706.0,1.0,1.0]) |\n|(54,[0,1,2,5,6,7,8,9,13,18],[1879.0,18.0,14.0,120.0,208.0,210.0,137.0,767.0,1.0,1.0])               |\n|(54,[0,1,2,3,4,5,6,7,8,9,13,15],[1879.0,23.0,18.0,108.0,28.0,134.0,207.0,200.0,124.0,875.0,1.0,1.0])|\n+----------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1513973600445_2040276865","id":"20171222-221320_1582651952","dateCreated":"2017-12-22T22:13:20+0200","dateStarted":"2017-12-25T00:55:05+0200","dateFinished":"2017-12-25T00:55:49+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1303"},{"text":"trainData.head","user":"admin","dateUpdated":"2017-12-25T00:03:32+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nres111: org.apache.spark.sql.Row = [1859,18,12,67,11,90,211,215,139,792,0,0,0,1,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,3.0]\n"}]},"apps":[],"jobName":"paragraph_1513975036360_-985383029","id":"20171222-223716_1722756529","dateCreated":"2017-12-22T22:37:16+0200","dateStarted":"2017-12-25T00:03:32+0200","dateFinished":"2017-12-25T00:04:05+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1304"},{"title":"İlk Karar Ağacı Modeli","text":"// Şimdi karar ağacı modeli oluşturmak için hazırız\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\nimport scala.util.Random\n\n// Model için DecisionTreeClassifier nesnesi oluşturup hedef değişken ismi ve girdi vector ismini belirliyoruz.\nval classifier = new DecisionTreeClassifier().setSeed(Random.nextLong()).setLabelCol(\"Cover_Type\").setFeaturesCol(\"featureVector\").setPredictionCol(\"prediction\")\n\n// sınıflandırıcı nesnemiz olan classifier ile (artık o bir makine/model/sınıflandırıcı ne derseniz) son dataframimiz olan assembledTrainData kullanarak (eğitim için, başka kötü maksatlarla değil) model adında model oluşturuyoruz.\n// Model oluşturmak ne demek burada biraz duralım: Eğitim setinin sırrını çözmek, çiğerine inmek, örüntülerini sökmek, genel temayüllerine nail olmak vb. ne derseniz deyin. Modelin sınıfı DecisionTreeClassificationModel  olacak\nval model = classifier.fit(assembledTrainData)\n\n// Karar ağacının karar aşamalarını yazdır\nprintln(model.toDebugString)","user":"admin","dateUpdated":"2018-01-07T20:25:13+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.classification.DecisionTreeClassifier\n\nimport scala.util.Random\n\nclassifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_e05732829432\n\nmodel: org.apache.spark.ml.classification.DecisionTreeClassificationModel = DecisionTreeClassificationModel (uid=dtc_e05732829432) of depth 5 with 63 nodes\nDecisionTreeClassificationModel (uid=dtc_e05732829432) of depth 5 with 63 nodes\n  If (feature 0 <= 3035.0)\n   If (feature 0 <= 2559.0)\n    If (feature 10 <= 0.0)\n     If (feature 0 <= 2457.0)\n      If (feature 3 <= 0.0)\n       Predict: 4.0\n      Else (feature 3 > 0.0)\n       Predict: 3.0\n     Else (feature 0 > 2457.0)\n      If (feature 17 <= 0.0)\n       Predict: 2.0\n      Else (feature 17 > 0.0)\n       Predict: 3.0\n    Else (feature 10 > 0.0)\n     If (feature 9 <= 4656.0)\n      If (feature 22 <= 0.0)\n       Predict: 2.0\n      Else (feature 22 > 0.0)\n       Predict: 2.0\n     Else (feature 9 > 4656.0)\n      If (feature 5 <= 900.0)\n       Predict: 2.0\n      Else (feature 5 > 900.0)\n       Predict: 2.0\n   Else (feature 0 > 2559.0)\n    If (feature 15 <= 0.0)\n     If (feature 0 <= 2942.0)\n      If (feature 17 <= 0.0)\n       Predict: 2.0\n      Else (feature 17 > 0.0)\n       Predict: 3.0\n     Else (feature 0 > 2942.0)\n      If (feature 3 <= 162.0)\n       Predict: 2.0\n      Else (feature 3 > 162.0)\n       Predict: 2.0\n    Else (feature 15 > 0.0)\n     If (feature 9 <= 1359.0)\n      If (feature 7 <= 214.0)\n       Predict: 2.0\n      Else (feature 7 > 214.0)\n       Predict: 3.0\n     Else (feature 9 > 1359.0)\n      If (feature 3 <= 210.0)\n       Predict: 2.0\n      Else (feature 3 > 210.0)\n       Predict: 3.0\n  Else (feature 0 > 3035.0)\n   If (feature 0 <= 3316.0)\n    If (feature 7 <= 239.0)\n     If (feature 0 <= 3099.0)\n      If (feature 3 <= 190.0)\n       Predict: 1.0\n      Else (feature 3 > 190.0)\n       Predict: 2.0\n     Else (feature 0 > 3099.0)\n      If (feature 5 <= 1110.0)\n       Predict: 1.0\n      Else (feature 5 > 1110.0)\n       Predict: 1.0\n    Else (feature 7 > 239.0)\n     If (feature 3 <= 362.0)\n      If (feature 0 <= 3185.0)\n       Predict: 2.0\n      Else (feature 0 > 3185.0)\n       Predict: 1.0\n     Else (feature 3 > 362.0)\n      If (feature 0 <= 3207.0)\n       Predict: 2.0\n      Else (feature 0 > 3207.0)\n       Predict: 2.0\n   Else (feature 0 > 3316.0)\n    If (feature 12 <= 0.0)\n     If (feature 4 <= 33.0)\n      If (feature 9 <= 2697.0)\n       Predict: 7.0\n      Else (feature 9 > 2697.0)\n       Predict: 1.0\n     Else (feature 4 > 33.0)\n      If (feature 10 <= 0.0)\n       Predict: 1.0\n      Else (feature 10 > 0.0)\n       Predict: 1.0\n    Else (feature 12 > 0.0)\n     If (feature 45 <= 0.0)\n      If (feature 0 <= 3371.0)\n       Predict: 7.0\n      Else (feature 0 > 3371.0)\n       Predict: 7.0\n     Else (feature 45 > 0.0)\n      If (feature 5 <= 999.0)\n       Predict: 7.0\n      Else (feature 5 > 999.0)\n       Predict: 1.0\n\n"}]},"apps":[],"jobName":"paragraph_1513975038706_1790995683","id":"20171222-223718_1993136947","dateCreated":"2017-12-22T22:37:18+0200","dateStarted":"2017-12-25T01:08:50+0200","dateFinished":"2017-12-25T01:10:22+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1305"},{"title":"Niteliklerin önem dereceleri","text":"// Yukarıda oluşan DecisionTreeClassificationModel  bir dönüştürücü\n// Karar ağaçları girdi niteliklerin karara ne kadar etki ettiğini. Doğru bir tahmin için her bir niteliğin katkısı. \n// Biz featureVector ile eğitmiştik onun gerçek niteliklerle bağlantısını kurmak için zip() metodunu kullanıyor ve gerçek sütun isimlerini de parametre veriyoruz.\n// Aşağıdaki nitelik, etki tuple larından puanı en yüksek olanın etkisi en çok anlamındadır. En çok etki Elevation iken bazı niteliklerin hiç bir etkisinin olmadığı görülüyor.\nmodel.featureImportances.toArray.zip(inputCols).sorted.reverse.foreach(println)","user":"admin","dateUpdated":"2018-01-07T20:25:06+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"tableHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(0.7836745305359121,Elevation)\n(0.050431529007586226,Horizontal_Distance_To_Hyrology)\n(0.03281714615544068,Wilderness_Area_0)\n(0.032584409188361356,Hillshade_Noon)\n(0.02920305815052901,Soil_Type_3)\n(0.02219508319678004,Soil_Type_1)\n(0.016862718657099825,Soil_Type_31)\n(0.012423081171918494,Horizontal_Distance_To_Roadways)\n(0.01141683223397654,Wilderness_Area_2)\n(0.003990177718680935,Soil_Type_21)\n(0.0024282936999090853,Hillshade_3pm)\n(0.0017016755843741976,Horizontal_Distance_To_Fire_Points)\n(2.7146469943168544E-4,Soil_Type_8)\n(0.0,Wilderness_Area_3)\n(0.0,Wilderness_Area_1)\n(0.0,Vertical_Distance_To_Hyrology)\n(0.0,Soil_Type_9)\n(0.0,Soil_Type_7)\n(0.0,Soil_Type_6)\n(0.0,Soil_Type_5)\n(0.0,Soil_Type_4)\n(0.0,Soil_Type_39)\n(0.0,Soil_Type_38)\n(0.0,Soil_Type_37)\n(0.0,Soil_Type_36)\n(0.0,Soil_Type_35)\n(0.0,Soil_Type_34)\n(0.0,Soil_Type_33)\n(0.0,Soil_Type_32)\n(0.0,Soil_Type_30)\n(0.0,Soil_Type_29)\n(0.0,Soil_Type_28)\n(0.0,Soil_Type_27)\n(0.0,Soil_Type_26)\n(0.0,Soil_Type_25)\n(0.0,Soil_Type_24)\n(0.0,Soil_Type_23)\n(0.0,Soil_Type_22)\n(0.0,Soil_Type_20)\n(0.0,Soil_Type_2)\n(0.0,Soil_Type_19)\n(0.0,Soil_Type_18)\n(0.0,Soil_Type_17)\n(0.0,Soil_Type_16)\n(0.0,Soil_Type_15)\n(0.0,Soil_Type_14)\n(0.0,Soil_Type_13)\n(0.0,Soil_Type_12)\n(0.0,Soil_Type_11)\n(0.0,Soil_Type_10)\n(0.0,Soil_Type_0)\n(0.0,Slope)\n(0.0,Hillshade_9am)\n(0.0,Aspect)\n"}]},"apps":[],"jobName":"paragraph_1514015353539_-2116890507","id":"20171223-094913_838568825","dateCreated":"2017-12-23T09:49:13+0200","dateStarted":"2017-12-25T00:04:15+0200","dateFinished":"2017-12-25T00:05:41+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1306"},{"title":"Model tahminlerini bir görelim","text":"// Aşağıda probability sütununda hedef değişkenin her bir faklı değeri için hesaplanan olasılık değerleri bulunmaktadır. Ancak hedef değişkenimizde 7 farklı değer olmasına rağmen 8 farklı sonuç bulunuyor. İlk değer olan veya başka bir deyişle 0 indisinde bulunan \n// değeri dikkate almayacağız. 1 il 7 indisindeki değerler olasılık değerleridir. İlk 0.0 değeri vektörün kendi çalışmasıyla ilgilidir.\nval predictions = model.transform(assembledTrainData)\npredictions.select(\"Cover_Type\",\"prediction\",\"probability\").show(truncate = false)","user":"admin","dateUpdated":"2018-01-07T20:23:54+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\npredictions: org.apache.spark.sql.DataFrame = [Elevation: int, Aspect: int ... 57 more fields]\n+----------+----------+------------------------------------------------------------------------------------------------+\n|Cover_Type|prediction|probability                                                                                     |\n+----------+----------+------------------------------------------------------------------------------------------------+\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |4.0       |[0.0,0.0,0.04330708661417323,0.2795275590551181,0.4308836395450569,0.0,0.2462817147856518,0.0]  |\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n|6.0       |4.0       |[0.0,0.0,0.04330708661417323,0.2795275590551181,0.4308836395450569,0.0,0.2462817147856518,0.0]  |\n|3.0       |3.0       |[0.0,0.0,0.03499812753208729,0.628876859701086,0.050216184931740036,0.0,0.28590882783508664,0.0]|\n+----------+----------+------------------------------------------------------------------------------------------------+\nonly showing top 20 rows\n\n"}]},"apps":[],"jobName":"paragraph_1514015351985_1210818054","id":"20171223-094911_2094795767","dateCreated":"2017-12-23T09:49:11+0200","dateStarted":"2017-12-25T00:05:28+0200","dateFinished":"2017-12-25T00:06:10+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1307"},{"text":"%md\nYukarıdaki modelde probability diye bir sütun var. Burada hedef değişkenin her bir farklı değişkenine ait olma olasılıkları veriliyor. Ancak toplam 7 olası sonuç olması beklenirken probability sütununda 8 değer var. \nİlk indisteki 0.0'ı dikkate almamak gerekiyor. Kalan 1-7 arasını dikkate almak gerekiyor. Dikkat edilirse model tahminlerinde çok yanlış tahmin var. Bunun sebebi modelin hiper parametreler kullanılmadan oluşturulmasıdır. \n\nMulticlassClassificationEvaluator doğruluğu diğer metrikleri ve modelik tahmin kalitesini hesaplayabilir ","user":"admin","dateUpdated":"2018-01-07T20:26:02+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Yukarıdaki modelde probability diye bir sütun var. Burada hedef değişkenin her bir farklı değişkenine ait olma olasılıkları veriliyor. Ancak toplam 7 olası sonuç olması beklenirken probability sütununda 8 değer var.<br/>İlk indistei 0.0&rsquo;ı dikkate almamak gerekiyor. Kalan 1-7 arasını dikkate almak gerekiyor.<br/>MulticlassClassificationEvaluator doğruluğu iğer metrikleri ve modelik tahmin kalitesini hesaplayabilir</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514015349800_-1913758412","id":"20171223-094909_738964047","dateCreated":"2017-12-23T09:49:09+0200","dateStarted":"2017-12-25T00:02:12+0200","dateFinished":"2017-12-25T00:02:13+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1308"},{"text":"import org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\n// MulticlassClassificationEvaluator sınıfından nesne oluşturuyoruz. Cover_Type'ı hedef değişken, tahmin edilen sonuçları da prediction sütunu parametre olarak veriyoruz. \nval evaluator = new MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\")\n\n// Hata matrisiyle ilgili hesaplamak istediğimiz metrik için setMetrikName metoduna parametre veriyoruz. Aşağıda çok kullanılan metrik olan accuracy verdik. Kullanılan metod evaluate. Sonuç 0.69 gibi vasat bir rakam.\nevaluator.setMetricName(\"accuracy\").evaluate(predictions)\n\n// İkinci sefer aynı şekilde f1 hesaplıyorz. f1 neydi la? F1 = 2 * (precision * recall) / (precision + recall)\nevaluator.setMetricName(\"f1\").evaluate(predictions)","user":"admin","dateUpdated":"2017-12-25T20:36:45+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator\n\nevaluator: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_8a5279d80734\n\nres120: Double = 0.6980857548190721\n\nres121: Double = 0.6815352638882501\n"}]},"apps":[],"jobName":"paragraph_1513975037229_-1418225541","id":"20171222-223717_679015051","dateCreated":"2017-12-22T22:37:17+0200","dateStarted":"2017-12-25T00:05:41+0200","dateFinished":"2017-12-25T00:07:04+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1309"},{"title":"Hata Matrisi","text":"//Hata matrisi hala eski mllib RRD API'si ile olduğundan predictions dataframe'i rdd ye çevirip çalışacağız.Çevirme işi kolay olduğundan sıkıntı yok.\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n\n// predictions dataframeden hedef niteliği ve tahmin sonuçların seçi, ikisini de double türüne çevirip rdd olarak predictionRDD'ye atıyoruz.\nval predictionRDD = predictions.select(\"prediction\",\"Cover_Type\").as[(Double, Double)].rdd\n\n// MulticlassMetrics sınıfından bir nesne yaratıyoruz. Parametre olarak yeni oluşturduğumuz rdd'yi veriyoruz. Bu bize hata matrainini de bir özellik olarak içinde barındıran bir nesne verecek.\nval multiclassMetrics = new MulticlassMetrics(predictionRDD)\n\n// hata matrisini gösterelim\nmulticlassMetrics.confusionMatrix","user":"admin","dateUpdated":"2017-12-25T20:15:55+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.mllib.evaluation.MulticlassMetrics\n\npredictionRDD: org.apache.spark.rdd.RDD[(Double, Double)] = MapPartitionsRDD[1870] at rdd at <console>:76\n\nmulticlassMetrics: org.apache.spark.mllib.evaluation.MulticlassMetrics = org.apache.spark.mllib.evaluation.MulticlassMetrics@51acd02e\n\n\n\n\n\n\n\n\nres124: org.apache.spark.mllib.linalg.Matrix =\n134280.0  50619.0   160.0    0.0    0.0   0.0  5460.0\n56698.0   193511.0  4217.0   99.0   44.0  0.0  671.0\n0.0       5608.0    25809.0  639.0  0.0   0.0  0.0\n0.0       19.0      1475.0   985.0  0.0   0.0  0.0\n0.0       7838.0    675.0    0.0    69.0  0.0  0.0\n0.0       5974.0    9107.0   563.0  0.0   0.0  0.0\n7969.0    34.0      40.0     0.0    0.0   0.0  10463.0\n"}]},"apps":[],"jobName":"paragraph_1513973610081_-1777971220","id":"20171222-221330_807142709","dateCreated":"2017-12-22T22:13:30+0200","dateStarted":"2017-12-25T00:06:11+0200","dateFinished":"2017-12-25T00:07:52+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1310"},{"title":"Aslında bu hata matrisi dataframe API si ile de yazdırılabilir","text":"val confusionMatrix = predictions.groupBy(\"Cover_Type\").pivot(\"prediction\",(1 to 7)).count().na.fill(0.0).orderBy(\"Cover_Type\")\nconfusionMatrix.show()","user":"admin","dateUpdated":"2017-12-25T00:07:04+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nconfusionMatrix: org.apache.spark.sql.Dataset[org.apache.spark.sql.Row] = [Cover_Type: double, 1: bigint ... 6 more fields]\n+----------+------+------+-----+---+---+---+-----+\n|Cover_Type|     1|     2|    3|  4|  5|  6|    7|\n+----------+------+------+-----+---+---+---+-----+\n|       1.0|134280| 50619|  160|  0|  0|  0| 5460|\n|       2.0| 56698|193511| 4217| 99| 44|  0|  671|\n|       3.0|     0|  5608|25809|639|  0|  0|    0|\n|       4.0|     0|    19| 1475|985|  0|  0|    0|\n|       5.0|     0|  7838|  675|  0| 69|  0|    0|\n|       6.0|     0|  5974| 9107|563|  0|  0|    0|\n|       7.0|  7969|    34|   40|  0|  0|  0|10463|\n+----------+------+------+-----+---+---+---+-----+\n\n"}]},"apps":[],"jobName":"paragraph_1514039242676_1122525095","id":"20171223-162722_674941744","dateCreated":"2017-12-23T16:27:22+0200","dateStarted":"2017-12-25T00:07:04+0200","dateFinished":"2017-12-25T00:08:12+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1311"},{"title":"Baseline Accuracy nedir?","text":"// Acaba rastgele sınıflandırma yapılsa model başarısı nedir? Bizim eğittiğimiz model rastgele modelden ne kadar daha başarılı görmek için bunu bilmemiz lazım.Aşağıda accuracy %37 biz % 69 doğrulukla tahmin etmiştik.\nimport org.apache.spark.sql.DataFrame\n\n// Hedef değişkende 7 farklı değer var. Her bir değerin tekrarlanma sayısının dataframe toplam satırına bölümü o değerin doğru olma olasılığını verir. İşte bu fonksiyon her bir değer iin bunu hesaplar ve yedi farklı değer için bir dizi döndürür.\ndef classProbabilities(data: DataFrame): Array[Double] = {\n   // Toplam satır sayısını toplam değişkenine ata\n    val total = data.count()\n    // Hedef değişken Cover_Type'a göre gruplandır, say, double yap ve her satırı total'e böl. Zaten bir alt paragrafta görüldüğü gibi tek sütunlu ve 7 satırlı bişey. Her bir satır hedef değişkenin her bir benzersiz değerinin tekrarlanma sayısı.\n    data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(\"count\").as[Double].map(_/total).collect()\n}\n\n// Yukarıdaki fonksiyonu kullanarak eğitim setinin sınıf olasılıklarını hesapla ve trainPriorProbabilities değişkenine aktar.\nval trainPriorProbabilities = classProbabilities(trainData)\n\n// Yukarıdaki fonksiyonu kullanarak eğitim setinin sınıf olasılıklarını hesapla ve testPriorProbabilities değişkenine aktar.\nval testPriorProbabilities = classProbabilities(testData)\n\n// random accuracy\ntrainPriorProbabilities.zip(testPriorProbabilities).map{\n    case (trainProb, cvProb) => trainProb * cvProb\n}.sum","user":"admin","dateUpdated":"2017-12-25T21:46:46+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.sql.DataFrame\n\nclassProbabilities: (data: org.apache.spark.sql.DataFrame)Array[Double]\n\ntrainPriorProbabilities: Array[Double] = Array(0.36426296207071923, 0.4880063323811818, 0.06128949612447565, 0.00473972613216169, 0.016408362108193474, 0.029910558939708544, 0.035382562243559595)\n\ntestPriorProbabilities: Array[Double] = Array(0.3676922015658952, 0.4839271548304763, 0.06377401441727314, 0.004621805263339427, 0.015710688786948573, 0.02971406891318594, 0.03456006622288139)\n\nres128: Double = 0.37639612451625654\n"}]},"apps":[],"jobName":"paragraph_1514039247730_1800467981","id":"20171223-162727_354403083","dateCreated":"2017-12-23T16:27:27+0200","dateStarted":"2017-12-25T00:07:52+0200","dateFinished":"2017-12-25T00:09:01+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1312"},{"text":"data.groupBy(\"Cover_Type\").count().orderBy(\"Cover_Type\").select(\"count\").as[Double].show()","user":"admin","dateUpdated":"2017-12-25T21:30:44+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------+\n| count|\n+------+\n|211840|\n|283301|\n| 35754|\n|  2747|\n|  9493|\n| 17367|\n| 20510|\n+------+\n\n"}]},"apps":[],"jobName":"paragraph_1514230219680_1717915935","id":"20171225-213019_1323053943","dateCreated":"2017-12-25T21:30:19+0200","dateStarted":"2017-12-25T21:30:44+0200","dateFinished":"2017-12-25T21:31:42+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1313"},{"title":"Desicion Tree Hyperparameters","text":"%md\nKarar ağaçları seçmek için gerekli hiper parametreler farklı: maximum depth, maximum bins, impurity measure, minimum information gain\nmaximum depth: ağacın derinliğini sınırlar. Aşırı öğrenme için buna sınır koymak gerekir. Karar düğümlerine Spark ML'de bin deniyor.\nmaximum bins: \nImpurity için iki metrik: gini impurity ve entropy\nGini impurity: ","user":"admin","dateUpdated":"2017-12-25T22:43:31+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"markdown","editOnDblClick":true},"title":true,"editorMode":"ace/mode/markdown","editorHide":false,"tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<div class=\"markdown-body\">\n<p>Karar ağaçları seçmek için gerekli hiper parametreler farklı: maximum depth, maximum bins, impurity measure, minimum information gain<br/>maximum depth: ağacın derinliğini sınırlar. Aşırı öğrenme için buna sınır koymak gerekir. Karar düğümlerine Spark ML&rsquo;de bin deniyor.<br/>maximum bins:</p>\n</div>"}]},"apps":[],"jobName":"paragraph_1514039245822_-1229075506","id":"20171223-162725_1434482794","dateCreated":"2017-12-23T16:27:25+0200","dateStarted":"2017-12-25T22:31:37+0200","dateFinished":"2017-12-25T22:31:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1314"},{"title":"Tuning Decision TreeVectorAssembler ve DecisionTreeClassifier dan bir Pipeline oluşturma","text":"import org.apache.spark.ml.Pipeline\nval inputCols = trainData.columns.filter(_!=\"Cover_type\")\nval assembler = new VectorAssembler().setInputCols(inputCols).setOutputCol(\"featureVector\")\n\nval classifier = new DecisionTreeClassifier().setSeed(Random.nextLong()).setLabelCol(\"Cover_Type\").setFeaturesCol(\"featureVector\").setPredictionCol(\"prediction\")\n\nval pipeline = new Pipeline().setStages(Array(assembler, classifier))","user":"admin","dateUpdated":"2017-12-25T00:08:13+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.Pipeline\ninputCols: Array[String] = Array(Elevation, Aspect, Slope, Horizontal_Distance_To_Hyrology, Vertical_Distance_To_Hyrology, Horizontal_Distance_To_Roadways, Hillshade_9am, Hillshade_Noon, Hillshade_3pm, Horizontal_Distance_To_Fire_Points, Wilderness_Area_0, Wilderness_Area_1, Wilderness_Area_2, Wilderness_Area_3, Soil_Type_0, Soil_Type_1, Soil_Type_2, Soil_Type_3, Soil_Type_4, Soil_Type_5, Soil_Type_6, Soil_Type_7, Soil_Type_8, Soil_Type_9, Soil_Type_10, Soil_Type_11, Soil_Type_12, Soil_Type_13, Soil_Type_14, Soil_Type_15, Soil_Type_16, Soil_Type_17, Soil_Type_18, Soil_Type_19, Soil_Type_20, Soil_Type_21, Soil_Type_22, Soil_Type_23, Soil_Type_24, Soil_Type_25, Soil_Type_26, Soil_Type_27, Soil_Type_28, Soil_Type_29, Soil_Type_30, Soil_Type_31, Soil_Type_32, Soil_Type_33, Soil_Type_34, Soi...\nassembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_e3d4636124d6\n\nclassifier: org.apache.spark.ml.classification.DecisionTreeClassifier = dtc_7d6f0e5d0c2a\n\npipeline: org.apache.spark.ml.Pipeline = pipeline_23ca7bcde9ac\n"}]},"apps":[],"jobName":"paragraph_1514042427772_-2058815251","id":"20171223-172027_370863759","dateCreated":"2017-12-23T17:20:27+0200","dateStarted":"2017-12-25T00:08:13+0200","dateFinished":"2017-12-25T00:09:06+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1315"},{"text":"import org.apache.spark.ml.tuning.ParamGridBuilder\nval paramGrid = new ParamGridBuilder(). addGrid(classifier.impurity, Seq(\"gini\",\"entropy\")).\n                                        addGrid(classifier.maxDepth, Seq(1,20)).\n                                        addGrid(classifier.maxBins, Seq(40,300)).\n                                        addGrid(classifier.minInfoGain, Seq(0.0,0.05)).build()","user":"admin","dateUpdated":"2018-01-07T20:26:48+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala","tableHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.tuning.ParamGridBuilder\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nparamGrid: Array[org.apache.spark.ml.param.ParamMap] =\nArray({\n\tdtc_7d6f0e5d0c2a-impurity: gini,\n\tdtc_7d6f0e5d0c2a-maxBins: 40,\n\tdtc_7d6f0e5d0c2a-maxDepth: 1,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0\n}, {\n\tdtc_7d6f0e5d0c2a-impurity: entropy,\n\tdtc_7d6f0e5d0c2a-maxBins: 40,\n\tdtc_7d6f0e5d0c2a-maxDepth: 1,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0\n}, {\n\tdtc_7d6f0e5d0c2a-impurity: gini,\n\tdtc_7d6f0e5d0c2a-maxBins: 40,\n\tdtc_7d6f0e5d0c2a-maxDepth: 20,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0\n}, {\n\tdtc_7d6f0e5d0c2a-impurity: entropy,\n\tdtc_7d6f0e5d0c2a-maxBins: 40,\n\tdtc_7d6f0e5d0c2a-maxDepth: 20,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0\n}, {\n\tdtc_7d6f0e5d0c2a-impurity: gini,\n\tdtc_7d6f0e5d0c2a-maxBins: 300,\n\tdtc_7d6f0e5d0c2a-maxDepth: 1,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0\n}, {\n\tdtc_7d6f0e5d0c2a-impurity: entropy,\n\tdtc_7d6..."}]},"apps":[],"jobName":"paragraph_1514042424237_-645848724","id":"20171223-172024_1518595196","dateCreated":"2017-12-23T17:20:24+0200","dateStarted":"2017-12-25T00:09:02+0200","dateFinished":"2017-12-25T00:09:09+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1316"},{"text":"val multiClassEval = new MulticlassClassificationEvaluator().setLabelCol(\"Cover_Type\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")","user":"admin","dateUpdated":"2017-12-25T00:09:07+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nmultiClassEval: org.apache.spark.ml.evaluation.MulticlassClassificationEvaluator = mcEval_13c6384ef5d5\n"}]},"apps":[],"jobName":"paragraph_1514057864508_-1777665536","id":"20171223-213744_464730045","dateCreated":"2017-12-23T21:37:44+0200","dateStarted":"2017-12-25T00:09:07+0200","dateFinished":"2017-12-25T00:09:11+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1317"},{"text":"import org.apache.spark.ml.tuning.TrainValidationSplit\n\nval validator = new TrainValidationSplit().setSeed(Random.nextLong()).setEstimator(pipeline).setEvaluator(multiClassEval).setEstimatorParamMaps(paramGrid).setTrainRatio(0.9)\n\nval validatorModel = validator.fit(trainData)","user":"admin","dateUpdated":"2017-12-25T00:09:10+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.tuning.TrainValidationSplit\n\nvalidator: org.apache.spark.ml.tuning.TrainValidationSplit = tvs_5e326ccc67ac\n\nvalidatorModel: org.apache.spark.ml.tuning.TrainValidationSplitModel = tvs_5e326ccc67ac\n"}]},"apps":[],"jobName":"paragraph_1514057864255_-1875776505","id":"20171223-213744_1319420321","dateCreated":"2017-12-23T21:37:44+0200","dateStarted":"2017-12-25T00:09:10+0200","dateFinished":"2017-12-25T00:13:16+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1318"},{"text":"import org.apache.spark.ml.PipelineModel\nval bestModel = validatorModel.bestModel\nbestModel.asInstanceOf[PipelineModel].stages.last.extractParamMap","user":"admin","dateUpdated":"2017-12-25T00:09:11+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"\nimport org.apache.spark.ml.PipelineModel\n\nbestModel: org.apache.spark.ml.Model[_] = pipeline_23ca7bcde9ac\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nres133: org.apache.spark.ml.param.ParamMap =\n{\n\tdtc_7d6f0e5d0c2a-cacheNodeIds: false,\n\tdtc_7d6f0e5d0c2a-checkpointInterval: 10,\n\tdtc_7d6f0e5d0c2a-featuresCol: featureVector,\n\tdtc_7d6f0e5d0c2a-impurity: gini,\n\tdtc_7d6f0e5d0c2a-labelCol: Cover_Type,\n\tdtc_7d6f0e5d0c2a-maxBins: 40,\n\tdtc_7d6f0e5d0c2a-maxDepth: 20,\n\tdtc_7d6f0e5d0c2a-maxMemoryInMB: 256,\n\tdtc_7d6f0e5d0c2a-minInfoGain: 0.0,\n\tdtc_7d6f0e5d0c2a-minInstancesPerNode: 1,\n\tdtc_7d6f0e5d0c2a-predictionCol: prediction,\n\tdtc_7d6f0e5d0c2a-probabilityCol: probability,\n\tdtc_7d6f0e5d0c2a-rawPredictionCol: rawPrediction,\n\tdtc_7d6f0e5d0c2a-seed: 365491582176417043\n}\n"}]},"apps":[],"jobName":"paragraph_1514057863929_-1653391641","id":"20171223-213743_1098656178","dateCreated":"2017-12-23T21:37:43+0200","dateStarted":"2017-12-25T00:09:11+0200","dateFinished":"2017-12-25T00:13:32+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1319"},{"user":"admin","dateUpdated":"2017-12-25T00:02:13+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514042420553_1816174812","id":"20171223-172020_51995826","dateCreated":"2017-12-23T17:20:20+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1320"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514139838801_-163930044","id":"20171224-202358_1293568583","dateCreated":"2017-12-24T20:23:58+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1321"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514139833939_-1207769134","id":"20171224-202353_1459654168","dateCreated":"2017-12-24T20:23:53+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1322"},{"user":"admin","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1514139823991_1238849122","id":"20171224-202343_978118264","dateCreated":"2017-12-24T20:23:43+0200","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1323"}],"name":"SparkTraining/AdvancedAnalyticsSpark/Chapter4","id":"2D2VJ8MZF","angularObjects":{"2CHS8UYQQ:shared_process":[],"2C8A4SZ9T_livy2:shared_process":[],"2CK8A9MEG:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CY64P8KM:shared_process":[],"2CKEKWY8Z:shared_process":[]},"config":{"looknfeel":"default","personalizedMode":"false"},"info":{}}